{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Intentamos analizar de forma previa los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk \n",
    "from  sklearn.feature_extraction import  DictVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from scipy.sparse import csr_matrix, hstack,vstack\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "#from gensim import matutils\n",
    "#from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global variables\n",
    "NUM_TRAIN=1000\n",
    "MAX_COMPONENTS=2\n",
    "NUM_FRASES=0\n",
    "train=pd.DataFrame()\n",
    "test=pd.DataFrame()\n",
    "SPARSE=True\n",
    "NJOBS=2\n",
    "N_COMPONENTS=20\n",
    "LB_name= LabelBinarizer( sparse_output=SPARSE)\n",
    "LB_cat1= LabelBinarizer( sparse_output=SPARSE)\n",
    "LB_cat2= LabelBinarizer( sparse_output=SPARSE)\n",
    "LB_cat3= LabelBinarizer( sparse_output=SPARSE)\n",
    "LB_shipping= LabelBinarizer( sparse_output=SPARSE)\n",
    "LB_item_condition_id= LabelBinarizer( sparse_output=SPARSE)\n",
    "T_MODEL='MLPR'\n",
    "if T_MODEL=='LinearReg':\n",
    "    MODEL = LinearRegression(fit_intercept=True, normalize=False, copy_X=False, n_jobs=NJOBS)\n",
    "if T_MODEL=='RBF':    \n",
    "    MODEL = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
    "if T_MODEL=='Ridge':   \n",
    "    MODEL = Ridge(alpha=100)\n",
    "if T_MODEL=='MLPR':\n",
    "    MODEL = MLPRegressor(verbose=True,hidden_layer_sizes=(50,10,5 ),early_stopping=True)\n",
    "SKALER = MaxAbsScaler()\n",
    "#REDUCT  =  TruncatedSVD(n_components=MAX_COMPONENTS,algorithm='randomized', n_iter=5, random_state=0, tol=0.0)\n",
    "REDUCT = NMF(n_components=MAX_COMPONENTS, init='random', random_state=0)\n",
    "if SPARSE:\n",
    "    X_train = csr_matrix((3, 4))\n",
    "Y_train = np.asarray((5))\n",
    "ID_CAMPO='train_id'\n",
    "frases=pd.DataFrame()\n",
    "vect = TfidfVectorizer(min_df=5,ngram_range=(1,2),max_features=1800,stop_words='english')\n",
    "ldamodel = LatentDirichletAllocation(n_components=N_COMPONENTS,learning_method='online')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train read\n",
    "def read_files(icase):\n",
    "    global train,test\n",
    "    if icase == 'train':\n",
    "        train=pd.read_table('../input/train.tsv')\n",
    "        print(\"se lee \"+'../input/train.tsv'+' filas:',len(train))\n",
    "    else:\n",
    "        train=pd.read_table('../input/test.tsv')\n",
    "        print(\"se lee \"+'../input/test.tsv')\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carga_datos(icase):\n",
    "    global train,NUM_TRAIN,ID_CAMPO\n",
    "    if icase=='train':\n",
    "        if NUM_TRAIN != 0:\n",
    "            train=read_files('train')\n",
    "            print(\"LEN train\",len(train))\n",
    "            if NUM_TRAIN > 1:\n",
    "                    train=train.sample(n=NUM_TRAIN) \n",
    "            else:\n",
    "                np.random.seed(0)\n",
    "                msk = np.random.rand(len(train)) < NUM_TRAIN\n",
    "                if icase == 'train':\n",
    "                    train = train[msk]\n",
    "                else:\n",
    "                    train = train[~msk]\n",
    "            ID_CAMPO=\"train_id\"\n",
    "            train.reset_index(drop=True,inplace=True)\n",
    "        else:\n",
    "            train=read_files(icase)\n",
    "            ID_CAMPO=\"test_id\"\n",
    "        train.reindex()    \n",
    "    else:\n",
    "            train=read_files(icase)\n",
    "            ID_CAMPO=\"test_id\"      \n",
    "    print(\"LEN train\",len(train))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_data(data):\n",
    "    data.category_name.fillna(value = \"Other/Other/Other\", inplace = True)\n",
    "    data.brand_name.fillna(value = \"Unknown\", inplace = True)\n",
    "    data.item_description.fillna(value = \"No description yet\", inplace = True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentence_fit():\n",
    "    global frases,train,test,NUM_TRAIN,NUM_FRASES\n",
    "    print(\"vectorize_sentence_fit:Start vectorize_sentence_fit\")\n",
    "    print(\"vectorize_sentence_fit:LEN train\",len(train))\n",
    "    test=pd.read_table('../input/test.tsv')\n",
    "    if NUM_TRAIN != 0:\n",
    "        if NUM_TRAIN > 1:\n",
    "            test=test.head(NUM_TRAIN)\n",
    "        else:\n",
    "            test=test.sample(frac=NUM_TRAIN)\n",
    "    print(\"vectorize_sentence_fit:Read:test\",len(test))\n",
    "    test['item_description_upper']= test['item_description'].str.upper()\n",
    "    txt=train['item_description_upper'].str.cat(sep='.')+test['item_description_upper'].str.cat(sep='.') \n",
    "    del test\n",
    "    print(\"vectorize_sentence_fit:Star sent_tokenize\")\n",
    "    sent=nltk.sent_tokenize(txt)\n",
    "    print(\"vectorize_sentence_fit:Star FreqDist\")\n",
    "    fdist_sent = nltk.FreqDist(sent)\n",
    "    frases=pd.DataFrame(fdist_sent.most_common(len(fdist_sent)),columns=['Word', 'Frequency'])\n",
    "    frases['len']=frases['Word'].str.len()\n",
    "    frases=frases[frases['len']>2] \n",
    "    frases=frases[frases['Frequency']>5]\n",
    "    frases=frases.head(NUM_FRASES)\n",
    "    frases.reset_index(drop=True,inplace=True)\n",
    "    print(\"End vectorize_sentence_fit\",len(frases))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_LDA():\n",
    "    global vect,ldamodel,train\n",
    "    txtu=train['item_description_upper'].str.cat(sep='.')\n",
    "    print(\"Extraccion palabras, long \",len(txtu))\n",
    "    corpus1=nltk.word_tokenize(txtu)\n",
    "    print(\"len corpus1\",len(corpus1))\n",
    "    corpus1=set(corpus1)\n",
    "    print(\"len corpus1 set:\",len(corpus1))\n",
    "    del txtu\n",
    "#    fdist_sent = nltk.FreqDist(sent)\n",
    "    print(\"Tfdif Fit\")\n",
    "    X = vect.fit_transform(corpus1)\n",
    "    del corpus1\n",
    "    print(\"End Tfdif Fit\",X.shape)\n",
    "    print(\"END Extract features \")\n",
    "#    id2words={}\n",
    "#    for i,word in enumerate(vect.get_feature_names()):\n",
    "#        id2words[i]=word\n",
    "#    print(\"Start LDA\")\n",
    "#    corpus=matutils.Sparse2Corpus(X,documents_columns=False)\n",
    "    ldamodel.fit_transform(X)\n",
    "    print(\"SIze:\",ldamodel.components_.shape)\n",
    "    print(\"End LDA\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproceso():\n",
    "    global train\n",
    "    global Y_train,ID_CAMPO\n",
    "#split category\n",
    "    train=fill_missing_data(train)\n",
    "    train[[\"cat_level_1\",\"cat_level_2\",\"cat_level_3\"]]=train[\"category_name\"].str.split('/', expand=True,n=2)\n",
    "    del train[\"category_name\"]\n",
    "#Calcule Log price    \n",
    "    if ID_CAMPO==\"train_id\":\n",
    "        train[\"target\"] = np.log1p(train.price)\n",
    "        Y_train = np.asarray(train['target'],dtype=np.float)  \n",
    "    else:\n",
    "        Y_train = np.zeros((train.shape[0],), dtype=float)\n",
    "    train['item_description_upper']= train['item_description'].str.upper()    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizacion_fit():\n",
    "    global LV_name,LB_cat1,LB_cat2,LB_cat3,LB_shipping,LB_item_condition_id\n",
    "    global train\n",
    "    global X_train,NUM_FRASES\n",
    "    print(\"vectorizacion_fit:Name\")\n",
    "    LB_name.fit(train['name'])\n",
    "    print(\"vectorizacion_fit:cat_level_1\")\n",
    "    LB_cat1.fit(train['cat_level_1'].astype(str))\n",
    "    print(\"vectorizacion_fit:cat_level_2\")\n",
    "    LB_cat2.fit(train['cat_level_2'].astype(str))\n",
    "    print(\"vectorizacion_fit:cat_level_3\")\n",
    "    LB_cat3.fit(train['cat_level_3'].astype(str))\n",
    "    print(\"vectorizacion_fit:shipping\")\n",
    "    LB_shipping.fit(train['shipping'])  \n",
    "    print(\"vectorizacion_fit:item_condition_id\")\n",
    "    LB_item_condition_id.fit(train['item_condition_id'])\n",
    "    if NUM_FRASES!=0:\n",
    "        print(\"vectorizacion_fit:vectorize_sentence_fit\")\n",
    "        vectorize_sentence_fit()\n",
    "    if N_COMPONENTS != 0:\n",
    "        item_LDA()           \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization_fit():\n",
    "    global SKALER\n",
    "    SKALER.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization_transform():\n",
    "    global SKALER\n",
    "    SKALER.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduction_fit():\n",
    "    global REDUCT\n",
    "    global X_train\n",
    "    X=REDUCT.fit_transform(X_train)\n",
    "    X_train=X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduction_transform():\n",
    "    global REDUCT\n",
    "    global X_train\n",
    "    X_train=REDUCT.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature(X,Y):\n",
    "    global SPARSE\n",
    "    print(\"Antes de add\",X.shape,Y.shape)\n",
    "    try:\n",
    "        if SPARSE:\n",
    "            X1 =  hstack([X,Y])\n",
    "        else:\n",
    "            X1 =  np.hstack((X,X_train))\n",
    "    except:\n",
    "        print(\"Error en add_feature\")\n",
    "        return X\n",
    "    print(\"Despues de add\",X.shape,Y.shape,X1.shape)    \n",
    "    return X1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentence_transform():\n",
    "    global train,frases\n",
    "    longi=len(train)\n",
    "    X_train_sent=csr_matrix((longi,0), dtype=int)\n",
    "    print(\"vectorize_sentence_transform:Star map con frases=\",len(frases),len(train))\n",
    "    train[\"tokenized_sents\"] = train[\"item_description_upper\"].fillna(\"\").map(nltk.sent_tokenize)\n",
    "    def index_va(sentence):\n",
    "        try:\n",
    "            return train[train['tokenized_sents'].apply(lambda x: sentence in x)].index.values.tolist()\n",
    "        except:\n",
    "            []\n",
    "        return\n",
    "    print(\"vectorize_sentence_transform:Star index_va\")\n",
    "    frases['index_va']=frases[\"Word\"].apply(index_va)     \n",
    "    for index, row in frases.iterrows():\n",
    "        try:\n",
    "            if len(row['index_va'])!=0:\n",
    "                unos=[1 for i  in range(len(row['index_va']))]\n",
    "                filas=row['index_va']\n",
    "                columnas=[0 for i  in range(len(row['index_va']))]\n",
    "                Y=csr_matrix((unos,(filas,columnas)),shape=(longi,1))\n",
    "            else:\n",
    "                Y=csr_matrix(([0],([1],[1])),shape=(longi,1))            \n",
    "            X_train_sent=hstack((X_train_sent,Y))\n",
    "        except:\n",
    "            print(\"Error en vectorize_sentence_transform\",X_train_sent.shape,Y.shape) \n",
    "            raise \n",
    "    return X_train_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_LDA_transform():\n",
    "    global train,vect ,ldamodel \n",
    "    print(\"Start vectorize_LDA_transform\")\n",
    "    caso=np.matrix(train['item_description_upper'].\\\n",
    "          apply(lambda row:ldamodel.transform(vect.transform(set(nltk.word_tokenize(row)))).sum(axis=0)))\n",
    "    print(caso.shape)\n",
    "    X_train_lda=csr_matrix(caso)\n",
    "    print(\"End vectorize_LDA_transform\",X_train_lda.shape)\n",
    "    return X_train_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizacion_transform():\n",
    "    global LV_name,LB_cat1,LB_cat2,LB_cat3,LB_shipping,LB_item_condition_id\n",
    "    global train\n",
    "    global X_train,Y_train\n",
    "    global SPARSE\n",
    "    print(\"vectorizacion_transform:Name\")\n",
    "    X =  LB_name.transform(train['name'])\n",
    "    X_train= X\n",
    "    print(\"vectorizacion_transform:cat_level_1\",X_train.shape)\n",
    "    X =  LB_cat1.transform(train['cat_level_1'].astype(str))\n",
    "    X_train=add_feature(X,X_train)\n",
    "    print(\"vectorizacion_transform:cat_level_2\",X_train.shape)\n",
    "    X =  LB_cat2.transform(train['cat_level_2'].astype(str))\n",
    "    X_train=add_feature(X,X_train)\n",
    "    print(\"vectorizacion_transform:cat_level_3\",X_train.shape)\n",
    "    X =  LB_cat3.transform(train['cat_level_3'].astype(str))\n",
    "    X_train=add_feature(X,X_train)\n",
    "    print(\"vectorizacion_transform:shipping\",X_train.shape)\n",
    "    X =  LB_shipping.transform(train['shipping'])\n",
    "    X_train=add_feature(X,X_train) \n",
    "    print(\"vectorizacion_transform:item_condition_id\")\n",
    "    X =  LB_item_condition_id.transform(train['item_condition_id'])\n",
    "    X_train=add_feature(X,X_train)\n",
    "    if NUM_FRASES!=0:\n",
    "        print(\"vectorizacion_transform:vectorize_sentence_transform\")\n",
    "        X= vectorize_sentence_transform()\n",
    "        X_train=add_feature(X_train,X)\n",
    "    if N_COMPONENTS != 0:   \n",
    "        print(\"vectorize_LDA_transform\")\n",
    "        X=vectorize_LDA_transform()\n",
    "        X_train=add_feature(X_train,X)\n",
    "    del train\n",
    "    print(\"Dimension final \",X_train.shape)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizacion():\n",
    "    vectorizacion_fit()\n",
    "    vectorizacion_transform()\n",
    "    normalization_fit()\n",
    "    normalization_transform()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizacion_transform_norm():\n",
    "    vectorizacion_transform()\n",
    "    normalization_transform()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ajuste():\n",
    "    global MODEL\n",
    "    MODEL.fit(X_train,Y_train)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mpredict(X_train):\n",
    "    global MODEL\n",
    "    return MODEL.predict(X_train).clip(min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(preds):\n",
    "    global train,ID_CAMPO\n",
    "    carga_datos('test')\n",
    "    train[\"price\"] = np.expm1(preds)\n",
    "    train[[ID_CAMPO, \"price\"]].to_csv(\"submission_ridge.csv\", index = False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargardo Datos\n",
      "se lee ../input/train.tsv filas: 1482535\n",
      "LEN train 1482535\n",
      "LEN train 1000\n",
      "Preproseco\n",
      "Vectorización\n",
      "vectorizacion_fit:Name\n",
      "vectorizacion_fit:cat_level_1\n",
      "vectorizacion_fit:cat_level_2\n",
      "vectorizacion_fit:cat_level_3\n",
      "vectorizacion_fit:shipping\n",
      "vectorizacion_fit:item_condition_id\n",
      "Extraccion palabras, long  151234\n",
      "len corpus1 29976\n",
      "len corpus1 set: 5903\n",
      "Tfdif Fit\n",
      "End Tfdif Fit (5903, 90)\n",
      "END Extract features \n",
      "SIze: (20, 90)\n",
      "End LDA\n",
      "vectorizacion_transform:Name\n",
      "vectorizacion_transform:cat_level_1 (1000, 995)\n",
      "Antes de add (1000, 10) (1000, 995)\n",
      "Despues de add (1000, 10) (1000, 995) (1000, 1005)\n",
      "vectorizacion_transform:cat_level_2 (1000, 1005)\n",
      "Antes de add (1000, 80) (1000, 1005)\n",
      "Despues de add (1000, 80) (1000, 1005) (1000, 1085)\n",
      "vectorizacion_transform:cat_level_3 (1000, 1085)\n",
      "Antes de add (1000, 227) (1000, 1085)\n",
      "Despues de add (1000, 227) (1000, 1085) (1000, 1312)\n",
      "vectorizacion_transform:shipping (1000, 1312)\n",
      "Antes de add (1000, 1) (1000, 1312)\n",
      "Despues de add (1000, 1) (1000, 1312) (1000, 1313)\n",
      "vectorizacion_transform:item_condition_id\n",
      "Antes de add (1000, 5) (1000, 1313)\n",
      "Despues de add (1000, 5) (1000, 1313) (1000, 1318)\n",
      "vectorize_LDA_transform\n",
      "Start vectorize_LDA_transform\n",
      "(1, 1000)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-cef363445e7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpreproceso\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Vectorización\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mvectorizacion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fin Vectorización\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-e2da2e75a887>\u001b[0m in \u001b[0;36mvectorizacion\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mvectorizacion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mvectorizacion_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mvectorizacion_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mnormalization_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mnormalization_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-48f441462453>\u001b[0m in \u001b[0;36mvectorizacion_transform\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mN_COMPONENTS\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"vectorize_LDA_transform\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvectorize_LDA_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madd_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-357b82351aa6>\u001b[0m in \u001b[0;36mvectorize_LDA_transform\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mcaso\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'item_description_upper'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m          \u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcaso\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mX_train_lda\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcaso\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"End vectorize_LDA_transform\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train_lda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX_train_lda\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mlc\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m     77\u001b[0m                         self.format)\n\u001b[0;32m     78\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcoo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcoo_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_self\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoo_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;31m# Read matrix dimensions given, if any\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mlc\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\scipy\\sparse\\coo.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m    182\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 184\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    185\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_canonical_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "print(\"Cargardo Datos\")\n",
    "carga_datos('train')\n",
    "print(\"Preproseco\")\n",
    "preproceso()\n",
    "print(\"Vectorización\")\n",
    "vectorizacion()\n",
    "print(\"Fin Vectorización\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-52-1cecb78861d2>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-52-1cecb78861d2>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    train['item_description_upper'].apply(lambda row:ldamodel.transform(vect.transform(set(nltk.word_tokenize(row))).sum(axis=0))).tolist()\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\n",
    "  train['item_description_upper'].apply(lambda row:ldamodel.transform(vect.transform(set(nltk.word_tokenize(row))).sum(axis=0))).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matrix(train['item_description_upper'].\\\n",
    "          apply(lambda row:ldamodel.transform(vect.transform(set(nltk.word_tokenize(row)))).sum(axis=0)).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train.shape,Y_train.shape\",X_train.shape,Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ajuste\")\n",
    "ajuste()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Error Fit:\",mean_squared_log_error(Y_train,mpredict(X_train)))\n",
    "except:\n",
    "    print(\"NO LOG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Carga datos test\")\n",
    "carga_datos('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preproceso test\")\n",
    "preproceso()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vectorización test\")\n",
    "vectorizacion_transform_norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train.shape,Y_train.shape\",X_train.shape,Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Prediccion test\")\n",
    "mpredict(X_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Error zeros:\",mean_squared_log_error(Y_train,mpredict(X_train)))\n",
    "except:\n",
    "    print(\"NO LOG TEST\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Save test\")\n",
    "save_result(mpredict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
