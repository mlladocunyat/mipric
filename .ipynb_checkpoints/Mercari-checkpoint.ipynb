{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Intentamos analizar de forma previa los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk \n",
    "from  sklearn.feature_extraction import  DictVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from scipy.sparse import csr_matrix, hstack,vstack\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global variables\n",
    "NUM_TRAIN=.5\n",
    "MAX_COMPONENTS=2\n",
    "NUM_FRASES=500\n",
    "train=pd.DataFrame()\n",
    "test=pd.DataFrame()\n",
    "SPARSE=True\n",
    "NJOBS=2\n",
    "LB_name= LabelBinarizer( sparse_output=SPARSE)\n",
    "LB_cat1= LabelBinarizer( sparse_output=SPARSE)\n",
    "LB_cat2= LabelBinarizer( sparse_output=SPARSE)\n",
    "LB_cat3= LabelBinarizer( sparse_output=SPARSE)\n",
    "LB_shipping= LabelBinarizer( sparse_output=SPARSE)\n",
    "LB_item_condition_id= LabelBinarizer( sparse_output=SPARSE)\n",
    "T_MODEL='MLPR'\n",
    "if T_MODEL=='LinearReg':\n",
    "    MODEL = LinearRegression(fit_intercept=True, normalize=False, copy_X=False, n_jobs=NJOBS)\n",
    "if T_MODEL=='RBF':    \n",
    "    MODEL = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
    "if T_MODEL=='Ridge':   \n",
    "    MODEL = Ridge(alpha=100)\n",
    "if T_MODEL=='MLPR':\n",
    "    MODEL = MLPRegressor(verbose=True,hidden_layer_sizes=(50,10,5 ),early_stopping=True)\n",
    "SKALER = MaxAbsScaler()\n",
    "#REDUCT  =  TruncatedSVD(n_components=MAX_COMPONENTS,algorithm='randomized', n_iter=5, random_state=0, tol=0.0)\n",
    "REDUCT = NMF(n_components=MAX_COMPONENTS, init='random', random_state=0)\n",
    "if SPARSE:\n",
    "    X_train = csr_matrix((3, 4))\n",
    "Y_train = np.asarray((5))\n",
    "ID_CAMPO='train_id'\n",
    "frases=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train read\n",
    "def read_files(icase):\n",
    "    global train,test\n",
    "    if icase == 'train':\n",
    "        train=pd.read_table('../input/train.tsv')\n",
    "        print(\"se lee \"+'../input/train.tsv'+' filas:',len(train))\n",
    "    else:\n",
    "        train=pd.read_table('../input/test.tsv')\n",
    "        print(\"se lee \"+'../input/test.tsv')\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carga_datos(icase):\n",
    "    global train,NUM_TRAIN,ID_CAMPO\n",
    "    if icase=='train':\n",
    "        if NUM_TRAIN != 0:\n",
    "            train=read_files('train')\n",
    "            print(\"LEN train\",len(train))\n",
    "            if NUM_TRAIN > 1:\n",
    "                    train=train.sample(n=NUM_TRAIN) \n",
    "            else:\n",
    "                np.random.seed(0)\n",
    "                msk = np.random.rand(len(train)) < NUM_TRAIN\n",
    "                if icase == 'train':\n",
    "                    train = train[msk]\n",
    "                else:\n",
    "                    train = train[~msk]\n",
    "            ID_CAMPO=\"train_id\"\n",
    "            train.reset_index(drop=True,inplace=True)\n",
    "        else:\n",
    "            train=read_files(icase)\n",
    "            ID_CAMPO=\"test_id\"\n",
    "        train.reindex()    \n",
    "    else:\n",
    "            train=read_files(icase)\n",
    "            ID_CAMPO=\"test_id\"      \n",
    "    print(\"LEN train\",len(train))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_data(data):\n",
    "    data.category_name.fillna(value = \"Other/Other/Other\", inplace = True)\n",
    "    data.brand_name.fillna(value = \"Unknown\", inplace = True)\n",
    "    data.item_description.fillna(value = \"No description yet\", inplace = True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentence_fit():\n",
    "    global frases,train,test,NUM_TRAIN,NUM_FRASES\n",
    "    print(\"vectorize_sentence_fit:Start vectorize_sentence_fit\")\n",
    "    print(\"vectorize_sentence_fit:LEN train\",len(train))\n",
    "    test=pd.read_table('../input/test.tsv')\n",
    "    if NUM_TRAIN != 0:\n",
    "        if NUM_TRAIN > 1:\n",
    "            test=test.head(NUM_TRAIN)\n",
    "        else:\n",
    "            test=test.sample(frac=NUM_TRAIN)\n",
    "    print(\"vectorize_sentence_fit:Read:test\",len(test))\n",
    "    test['item_description_upper']= test['item_description'].str.upper()\n",
    "    txt=train['item_description_upper'].str.cat(sep='.')+test['item_description_upper'].str.cat(sep='.') \n",
    "    del test\n",
    "    print(\"vectorize_sentence_fit:Star sent_tokenize\")\n",
    "    sent=nltk.sent_tokenize(txt)\n",
    "    print(\"vectorize_sentence_fit:Star FreqDist\")\n",
    "    fdist_sent = nltk.FreqDist(sent)\n",
    "    frases=pd.DataFrame(fdist_sent.most_common(len(fdist_sent)),columns=['Word', 'Frequency'])\n",
    "    frases['len']=frases['Word'].str.len()\n",
    "    frases=frases[frases['len']>2] \n",
    "    frases=frases[frases['Frequency']>5]\n",
    "    frases=frases.head(NUM_FRASES)\n",
    "    frases.reset_index(drop=True,inplace=True)\n",
    "    print(\"End vectorize_sentence_fit\",len(frases))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproceso():\n",
    "    global train\n",
    "    global Y_train,ID_CAMPO\n",
    "#split category\n",
    "    train=fill_missing_data(train)\n",
    "    train[[\"cat_level_1\",\"cat_level_2\",\"cat_level_3\"]]=train[\"category_name\"].str.split('/', expand=True,n=2)\n",
    "    del train[\"category_name\"]\n",
    "#Calcule Log price    \n",
    "    if ID_CAMPO==\"train_id\":\n",
    "        train[\"target\"] = np.log1p(train.price)\n",
    "        Y_train = np.asarray(train['target'],dtype=np.float)  \n",
    "    else:\n",
    "        Y_train = np.zeros((train.shape[0],), dtype=float)\n",
    "    train['item_description_upper']= train['item_description'].str.upper()    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizacion_fit():\n",
    "    global LV_name,LB_cat1,LB_cat2,LB_cat3,LB_shipping,LB_item_condition_id\n",
    "    global train\n",
    "    global X_train,NUM_FRASES\n",
    "    print(\"vectorizacion_fit:Name\")\n",
    "    LB_name.fit(train['name'])\n",
    "    print(\"vectorizacion_fit:cat_level_1\")\n",
    "    LB_cat1.fit(train['cat_level_1'].astype(str))\n",
    "    print(\"vectorizacion_fit:cat_level_2\")\n",
    "    LB_cat2.fit(train['cat_level_2'].astype(str))\n",
    "    print(\"vectorizacion_fit:cat_level_3\")\n",
    "    LB_cat3.fit(train['cat_level_3'].astype(str))\n",
    "    print(\"vectorizacion_fit:shipping\")\n",
    "    LB_shipping.fit(train['shipping'])  \n",
    "    print(\"vectorizacion_fit:item_condition_id\")\n",
    "    LB_item_condition_id.fit(train['item_condition_id'])\n",
    "    if NUM_FRASES!=0:\n",
    "        print(\"vectorizacion_fit:vectorize_sentence_fit\")\n",
    "        vectorize_sentence_fit()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization_fit():\n",
    "    global SKALER\n",
    "    SKALER.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization_transform():\n",
    "    global SKALER\n",
    "    SKALER.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduction_fit():\n",
    "    global REDUCT\n",
    "    global X_train\n",
    "    X=REDUCT.fit_transform(X_train)\n",
    "    X_train=X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduction_transform():\n",
    "    global REDUCT\n",
    "    global X_train\n",
    "    X_train=REDUCT.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature(X,Y):\n",
    "    global SPARSE\n",
    "    print(\"Antes de add\",X.shape,Y.shape)\n",
    "    try:\n",
    "        if SPARSE:\n",
    "            X1 =  hstack([X,Y])\n",
    "        else:\n",
    "            X1 =  np.hstack((X,X_train))\n",
    "    except:\n",
    "        print(\"Error en add_feature\")\n",
    "        return X\n",
    "    print(\"Despues de add\",X.shape,Y.shape,X1.shape)    \n",
    "    return X1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentence_transform():\n",
    "    global train,frases\n",
    "    longi=len(train)\n",
    "    X_train_sent=csr_matrix((longi,0), dtype=int)\n",
    "    print(\"vectorize_sentence_transform:Star map con frases=\",len(frases),len(train))\n",
    "    train[\"tokenized_sents\"] = train[\"item_description_upper\"].fillna(\"\").map(nltk.sent_tokenize)\n",
    "    def index_va(sentence):\n",
    "        try:\n",
    "            return train[train['tokenized_sents'].apply(lambda x: sentence in x)].index.values.tolist()\n",
    "        except:\n",
    "            []\n",
    "        return\n",
    "    print(\"vectorize_sentence_transform:Star index_va\")\n",
    "    frases['index_va']=frases[\"Word\"].apply(index_va)     \n",
    "    for index, row in frases.iterrows():\n",
    "        try:\n",
    "            if len(row['index_va'])!=0:\n",
    "                unos=[1 for i  in range(len(row['index_va']))]\n",
    "                filas=row['index_va']\n",
    "                columnas=[0 for i  in range(len(row['index_va']))]\n",
    "                Y=csr_matrix((unos,(filas,columnas)),shape=(longi,1))\n",
    "            else:\n",
    "                Y=csr_matrix(([0],([1],[1])),shape=(longi,1))            \n",
    "            X_train_sent=hstack((X_train_sent,Y))\n",
    "        except:\n",
    "            print(\"Error en vectorize_sentence_transform\",X_train_sent.shape,Y.shape) \n",
    "            raise \n",
    "    return X_train_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizacion_transform():\n",
    "    global LV_name,LB_cat1,LB_cat2,LB_cat3,LB_shipping,LB_item_condition_id\n",
    "    global train\n",
    "    global X_train,Y_train\n",
    "    global SPARSE\n",
    "    print(\"vectorizacion_transform:Name\")\n",
    "    X =  LB_name.transform(train['name'])\n",
    "    X_train= X\n",
    "    print(\"vectorizacion_transform:cat_level_1\")\n",
    "    X =  LB_cat1.transform(train['cat_level_1'].astype(str))\n",
    "    X_train=add_feature(X,X_train)\n",
    "    print(\"vectorizacion_transform:cat_level_2\")\n",
    "    X =  LB_cat2.transform(train['cat_level_2'].astype(str))\n",
    "    X_train=add_feature(X,X_train)\n",
    "    print(\"vectorizacion_transform:cat_level_3\")\n",
    "    X =  LB_cat3.transform(train['cat_level_3'].astype(str))\n",
    "    X_train=add_feature(X,X_train)\n",
    "    print(\"vectorizacion_transform:shipping\")\n",
    "    X =  LB_shipping.transform(train['shipping'])\n",
    "    X_train=add_feature(X,X_train) \n",
    "    print(\"vectorizacion_transform:item_condition_id\")\n",
    "    X =  LB_item_condition_id.transform(train['item_condition_id'])\n",
    "    X_train=add_feature(X,X_train)\n",
    "    if NUM_FRASES!=0:\n",
    "        print(\"vectorizacion_transform:vectorize_sentence_transform\")\n",
    "        X= vectorize_sentence_transform()\n",
    "        X_train=add_feature(X_train,X)\n",
    "    del train\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizacion():\n",
    "    vectorizacion_fit()\n",
    "    vectorizacion_transform()\n",
    "    normalization_fit()\n",
    "    normalization_transform()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizacion_transform_norm():\n",
    "    vectorizacion_transform()\n",
    "    normalization_transform()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ajuste():\n",
    "    global MODEL\n",
    "    MODEL.fit(X_train,Y_train)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mpredict(X_train):\n",
    "    global MODEL\n",
    "    return MODEL.predict(X_train).clip(min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(preds):\n",
    "    global train,ID_CAMPO\n",
    "    carga_datos('test')\n",
    "    train[\"price\"] = np.expm1(preds)\n",
    "    train[[ID_CAMPO, \"price\"]].to_csv(\"submission_ridge.csv\", index = False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargardo Datos\n",
      "se lee ../input/train.tsv filas: 1482535\n",
      "LEN train 1482535\n",
      "LEN train 740556\n",
      "Preproseco\n",
      "Vectorización\n",
      "vectorizacion_fit:Name\n",
      "vectorizacion_fit:cat_level_1\n",
      "vectorizacion_fit:cat_level_2\n",
      "vectorizacion_fit:cat_level_3\n",
      "vectorizacion_fit:shipping\n",
      "vectorizacion_fit:item_condition_id\n",
      "vectorizacion_transform:Name\n",
      "vectorizacion_transform:cat_level_1\n",
      "Antes de add (740556, 10) (740556, 638638)\n",
      "Despues de add (740556, 10) (740556, 638638) (740556, 638648)\n",
      "vectorizacion_transform:cat_level_2\n",
      "Antes de add (740556, 113) (740556, 638648)\n",
      "Despues de add (740556, 113) (740556, 638648) (740556, 638761)\n",
      "vectorizacion_transform:cat_level_3\n",
      "Antes de add (740556, 841) (740556, 638761)\n",
      "Despues de add (740556, 841) (740556, 638761) (740556, 639602)\n",
      "vectorizacion_transform:shipping\n",
      "Antes de add (740556, 1) (740556, 639602)\n",
      "Despues de add (740556, 1) (740556, 639602) (740556, 639603)\n",
      "vectorizacion_transform:item_condition_id\n",
      "Antes de add (740556, 5) (740556, 639603)\n",
      "Despues de add (740556, 5) (740556, 639603) (740556, 639608)\n",
      "Fin Vectorización\n"
     ]
    }
   ],
   "source": [
    "print(\"Cargardo Datos\")\n",
    "carga_datos('train')\n",
    "print(\"Preproseco\")\n",
    "preproceso()\n",
    "print(\"Vectorización\")\n",
    "vectorizacion()\n",
    "print(\"Fin Vectorización\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape,Y_train.shape (740556, 639608) (740556,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train.shape,Y_train.shape\",X_train.shape,Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ajuste\n"
     ]
    }
   ],
   "source": [
    "print(\"Ajuste\")\n",
    "ajuste()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Error Fit:\",mean_squared_log_error(Y_train,mpredict(X_train)))\n",
    "except:\n",
    "    print(\"NO LOG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Carga datos test\")\n",
    "carga_datos('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preproceso test\")\n",
    "preproceso()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vectorización test\")\n",
    "vectorizacion_transform_norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train.shape,Y_train.shape\",X_train.shape,Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Prediccion test\")\n",
    "mpredict(X_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Error zeros:\",mean_squared_log_error(Y_train,mpredict(X_train)))\n",
    "except:\n",
    "    print(\"NO LOG TEST\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Save test\")\n",
    "save_result(mpredict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
